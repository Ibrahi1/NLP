{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rFBQVgznApO"
      },
      "source": [
        "## 1. Vous pouvez utiliser un autre site web de votre choix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": true,
        "id": "EYI53mlrgTB-"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://fstt.ac.ma/Portail2023/\"\n",
        "\n",
        "headers = {\n",
        "  \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
        "  \"Accept-Language\": \"fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
        "  \"Accept-Encoding\": \"gzip, deflate, br\",\n",
        "  \"Connection\": \"keep-alive\"\n",
        "}\n",
        "\n",
        "data = []\n",
        "\n",
        "try:\n",
        "  response = requests.get(url, headers=headers)\n",
        "  response.raise_for_status() # VÃ©rifie si la requÃªte a rÃ©ussi\n",
        "except requests.exceptions.RequestException as e:\n",
        "  print(f\"Erreur de connexion : {e}\")\n",
        "else:\n",
        "  titres = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "  for title in titres.find_all(\"div\", class_=\"elementor-posts-container elementor-posts elementor-posts--skin-classic elementor-grid\"):\n",
        "    for article in title.find_all(\"article\"):\n",
        "      titres = article.h3.a\n",
        "      dates = article.div.span\n",
        "      if titres and titres.has_attr('href') and dates:\n",
        "        data.append((titres.text.strip(), titres['href'], dates.text.strip()))\n",
        "\n",
        "  time.sleep(5)\n",
        "\n",
        "\n",
        "# Step 2: Write to CSV\n",
        "with open('articles_BeautifulSoup.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Title', 'Link', 'Date'])\n",
        "    writer.writerows(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faZYWuuAnzMP"
      },
      "source": [
        "## 2. Scraping du contenu dynamique :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "inj3pO1Rn66G"
      },
      "outputs": [],
      "source": [
        "# !pip install selenium\n",
        "# !apt-get update\n",
        "# !apt install chromium-chromedriver\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.by import By\n",
        "import csv\n",
        "import time\n",
        "\n",
        "chromedriver_path = '/usr/bin/chromedriver'\n",
        "\n",
        "# Configuration des options Chrome driver\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# Initialiser le WebDriver\n",
        "driver = webdriver.Chrome(options=options)\n",
        "\n",
        "data = []\n",
        "\n",
        "# Changer le lien selon le besoin\n",
        "driver.get('https://fstt.ac.ma/Portail2023/')\n",
        "# Attendre qu'un Ã©lÃ©ment spÃ©cifique soit chargÃ©\n",
        "try:\n",
        "  wait = WebDriverWait(driver, 10)\n",
        "  wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"elementor-posts-container\")))\n",
        "  time.sleep(2)\n",
        "  articles = driver.find_elements(By.CLASS_NAME, \"elementor-post\")\n",
        "\n",
        "  for article in articles:\n",
        "    title = article.find_element(By.CLASS_NAME, \"elementor-post__title\").text.strip()\n",
        "    link = article.find_element(By.CLASS_NAME, \"elementor-post__title\").find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
        "    date = article.find_element(By.CLASS_NAME, \"elementor-post-date\").text.strip()\n",
        "    data.append((title, link, date))\n",
        "\n",
        "except Exception as e:\n",
        "  print(f\"Erreur lors de l'attente ou de l'extraction : {e}\")\n",
        "\n",
        "# Step 2: Write to CSV\n",
        "with open('articles_selenium.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Title', 'Link', 'Date'])\n",
        "    writer.writerows(data)\n",
        "\n",
        "# Fermer le navigateur\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1bSR_HNf9YM"
      },
      "source": [
        "## 3. Combinaison des deux :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "BvvcofSviNXP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Chargement des deux fichiers CSV\n",
        "bs_df = pd.read_csv(\"articles_BeautifulSoup.csv\")\n",
        "selenium_df = pd.read_csv(\"articles_selenium.csv\")\n",
        "\n",
        "# Fusion et suppression des doublons basÃ©s sur le lien\n",
        "combined_df = pd.concat([bs_df, selenium_df], ignore_index=True)\n",
        "combined_df.drop_duplicates(subset=\"Link\", inplace=True)\n",
        "\n",
        "# Enregistrement dans un nouveau fichier\n",
        "combined_df.to_csv(\"articles_combines.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtJLXSk6mInD"
      },
      "source": [
        "## 4. Bonus (+) :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EWd3gt3mOyT"
      },
      "source": [
        "### 1. GÃ©rer la pagination dynamique (boutons \"Suivant\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgmkA2unmKF6"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.by import By\n",
        "import csv\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "chromedriver_path = '/usr/bin/chromedriver'\n",
        "\n",
        "# Configuration des options Chrome driver\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# Initialiser le WebDriver\n",
        "driver = webdriver.Chrome(options=options)\n",
        "\n",
        "articlesPagination = []\n",
        "\n",
        "# Changer le lien selon le besoin\n",
        "driver.get('https://fstt.ac.ma/Portail2023/category/articles/actualites/')\n",
        "# Attendre qu'un Ã©lÃ©ment spÃ©cifique soit chargÃ©\n",
        "\n",
        "while True:\n",
        "    time.sleep(2)  # laisser le temps Ã  la page de charger\n",
        "\n",
        "    # SÃ©lectionner tous les articles sur la page\n",
        "    post_elements = driver.find_elements(By.CSS_SELECTOR, \".elementor-post\")\n",
        "\n",
        "    for post in post_elements:\n",
        "        try:\n",
        "            title = post.find_element(By.CSS_SELECTOR, \"h3.elementor-post__title\").text.strip()\n",
        "            link = post.find_element(By.CSS_SELECTOR, \"a\").get_attribute(\"href\")\n",
        "            date = post.find_element(By.CSS_SELECTOR, \"span.elementor-post-date\").text.strip()\n",
        "\n",
        "            articlesPagination.append((title, link, date))\n",
        "\n",
        "        except Exception as e:\n",
        "          print(f\"Erreur lors de l'attente ou de l'extraction : {e}\")\n",
        "\n",
        "    try:\n",
        "      next_button = driver.find_element(By.CSS_SELECTOR, \"a.next.page-numbers\")\n",
        "      next_button.click()\n",
        "    except Exception as e:\n",
        "      break\n",
        "\n",
        "# Fermer le navigateur\n",
        "driver.quit()\n",
        "\n",
        "# Sauvegarde dans un fichier CSV\n",
        "with open('articles_selenium_pagination.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Title', 'Link', 'Date'])\n",
        "    writer.writerows(articlesPagination)\n",
        "\n",
        "print(\"âœ… Scraping terminÃ© : articles_selenium_pagination.csv crÃ©Ã©.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTnMQuo43d26"
      },
      "source": [
        "### 2. Extraire des images ou d'autres mÃ©dias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "zZ5V2nctzIyt"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import csv\n",
        "import time\n",
        "\n",
        "# Setup headless Chrome\n",
        "options = Options()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "driver = webdriver.Chrome(options=options)\n",
        "\n",
        "# Open the main site\n",
        "driver.get('https://fstt.ac.ma/Portail2023/')\n",
        "time.sleep(3)\n",
        "\n",
        "media_data = []\n",
        "\n",
        "# ðŸ“· Images\n",
        "images = driver.find_elements(By.TAG_NAME, \"img\")\n",
        "for img in images:\n",
        "    src = img.get_attribute(\"src\")\n",
        "    if src:\n",
        "        media_data.append((\"image\", src))\n",
        "\n",
        "# ðŸŽ¥ Videos\n",
        "videos = driver.find_elements(By.TAG_NAME, \"video\")\n",
        "for video in videos:\n",
        "    src = video.get_attribute(\"src\")\n",
        "    if src:\n",
        "        media_data.append((\"video\", src))\n",
        "\n",
        "# ðŸ”Š Audio\n",
        "audios = driver.find_elements(By.TAG_NAME, \"audio\")\n",
        "for audio in audios:\n",
        "    src = audio.get_attribute(\"src\")\n",
        "    if src:\n",
        "        media_data.append((\"audio\", src))\n",
        "\n",
        "# ðŸ“¦ Iframes (e.g. YouTube embeds)\n",
        "iframes = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
        "for iframe in iframes:\n",
        "    src = iframe.get_attribute(\"src\")\n",
        "    if src and (\"youtube.com\" in src or \"player\" in src):\n",
        "        media_data.append((\"iframe\", src))\n",
        "\n",
        "# Save to CSV\n",
        "with open(\"media_from_fstt.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"Type\", \"URL\"])\n",
        "    writer.writerows(media_data)\n",
        "\n",
        "driver.quit()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "5rFBQVgznApO",
        "g1bSR_HNf9YM",
        "8EWd3gt3mOyT"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
